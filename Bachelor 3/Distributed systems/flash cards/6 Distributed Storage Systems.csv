"Explain why Mooreâ€™s law led to a paradigm shift from faster execution to parallel execution after 2005.", "Physical limits such as the speed of light, atomic boundaries, and restricted 3D chip layering meant CPUs could no longer increase clock speeds indefinitely, forcing the industry toward parallel execution using distributed and multicore systems."
"What is Hadoop, and why is it suitable for large-scale distributed storage and computation?", "Hadoop is an Apache open-source framework for reliable, scalable, distributed computing and storage across commodity hardware. It is suitable because it provides fault tolerance, high availability, and efficient handling of massive datasets."
"In a Google cluster with thousands of machines, failures are common. How does Hadoop handle such failures?", "Hadoop ensures fault tolerance by replicating data blocks in HDFS across multiple nodes and reassigning failed tasks in MapReduce to other healthy nodes."
"Describe the core principle behind Hadoop MapReduce in terms of 'moving computation to the data.'", "Rather than moving large data across the network, MapReduce runs computation on the machines where the data blocks reside, reducing network overhead and improving performance."
"In MapReduce, what is the role of the Mapper and Reducer functions?", "The Mapper transforms input key-value pairs into intermediate key-value pairs, often filtering or distributing data. The Reducer aggregates intermediate values by key into final output values."
"Provide a small example of how a word count program would work in MapReduce.", "The Mapper outputs <word,1> for each word; intermediate pairs are grouped by key; the Reducer sums the values for each word to produce <word,total_count>."
"What is the function of the Partitioner in MapReduce?", "The Partitioner decides which Reducer will process a particular key, ensuring that all values for the same key are sent to the same Reducer."
"Explain the main components of the Hadoop architecture: HDFS, YARN, and MapReduce.", "HDFS provides distributed file storage, MapReduce is the processing framework, and YARN manages cluster resources through the Resource Manager, Node Managers, and Application Masters."
"Describe the lifecycle of a YARN application from submission to completion.", "Steps: (1) client submits job, (2) Resource Manager allocates container for Application Master, (3) AM registers with Node Managers, (4) AM requests containers, (5) tasks are launched, (6) code executes, (7) status monitored, (8) AM unregisters upon completion."
"Compare Pig and Hive in the Hadoop ecosystem.", "Pig provides a scripting language for data transformation, while Hive offers an SQL-like interface for querying large datasets. Both compile high-level instructions into MapReduce jobs but serve different user preferences (procedural vs declarative)."
"What is Kafka, and how does it complement Hadoop?", "Kafka is a distributed commit log and messaging system. It complements Hadoop by streaming event data into HDFS or Spark for real-time or batch analytics."
"List two major shortcomings of the MapReduce model.", "(1) Inefficiency for iterative tasks (e.g., machine learning) because of disk-based data flow. (2) Limited expressiveness, since workflows must fit into the map and reduce pattern only."
"Explain how Apache Spark improves upon MapReduce.", "Spark performs in-memory computation, supports a wider set of operations beyond map/reduce, and is faster and more flexible for iterative, interactive, and streaming workloads."
"Define a Resilient Distributed Dataset (RDD).", "An RDD is an immutable distributed collection of objects in Spark that can be built from storage or transformed from other RDDs, with lineage tracking for fault tolerance."
"What are DataFrames in Spark, and how do they differ from RDDs?", "DataFrames organize data into named columns like a relational table, allow optimizations via SparkSQL, and are built on top of RDDs, offering higher-level abstractions."
"Explain the concept of Directed Acyclic Graphs (DAGs) in Spark execution.", "A DAG represents a sequence of transformations and their dependencies. Nodes are RDDs, and edges are transformations. DAGs enable Spark to optimize execution and recover data lineage."
"Differentiate between narrow and wide transformations in Spark.", "Narrow transformations require data from a single parent partition (e.g., map, filter), while wide transformations require shuffling data across multiple partitions (e.g., groupByKey, reduceByKey)."
"Provide an example of a Spark workflow using RDDs to perform word count.", "Text data is loaded into an RDD, split into words with flatMap, mapped to <word,1>, then reduced by key to sum counts. Results are saved to distributed storage."
"List three main use cases of Apache Spark and provide one real-world example.", "Use cases: (1) Machine learning, (2) Streaming data processing, (3) Interactive analytics. Example: Uber uses Spark with Kafka and HDFS for real-time ETL and optimization of ride operations."
"Under what circumstances might Spark not be the best choice?", "Spark is less suitable for simple batch tasks that can be efficiently handled by MapReduce or Hive, or when memory is insufficient for the dataset, since Spark requires careful memory management in multi-user environments."
