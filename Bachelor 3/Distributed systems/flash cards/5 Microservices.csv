"Explain the paradigm shift in computing performance around 2005 and its implications for distributed storage and processing.","Before 2005, performance improvements came from faster CPUs. Since 2005, physical limits (speed of light, atomic boundaries, limited 3D layering) forced parallel execution. This shift required distributed systems like Hadoop and Spark for scalability."
"What is Hadoop, and why is it significant in distributed storage and processing?","Hadoop is an Apache open-source framework providing scalable, reliable distributed storage (HDFS) and computation (MapReduce). It enables large-scale data processing on commodity hardware, ensuring fault tolerance and scalability."
"Describe three types of failures that commonly occur in large Google clusters and explain why distributed frameworks must handle them.","Examples include machine failures, rack failures, and router failures. Such failures are inevitable at scale, so frameworks like MapReduce and Hadoop include built-in fault tolerance mechanisms."
"What are the roles of the Mapper and Reducer in the MapReduce paradigm?","The Mapper transforms input key-value pairs into intermediate pairs, often filtering or distributing data. The Reducer aggregates or consolidates intermediate results into final outputs."
"Why is 'moving computation to the data' a core principle in Hadoop MapReduce?","Because moving large datasets across networks is costly, MapReduce runs tasks on nodes where the data already resides, reducing network overhead and improving efficiency."
"Describe the function of the Partitioner in a MapReduce job.","The Partitioner decides which Reducer a key-value pair is sent to, ensuring that all values for the same key end up at the same Reducer."
"Explain the role of YARN in the Hadoop ecosystem.","YARN (Yet Another Resource Negotiator) manages resources across the Hadoop cluster. It includes the Resource Manager (cluster-wide allocation), Node Manager (per-node monitoring), Application Master (per-application management), and containers for tasks."
"List and briefly explain three key components of the Hadoop ecosystem beyond HDFS and MapReduce.","Pig (scripting language for data analysis), Hive (SQL-like query interface), and Kafka (distributed commit log/message broker for real-time event streaming)."
"Provide an example of a real-world use case where Hadoop was applied successfully.","Spotify uses Hadoop clusters for data reporting, music trend analysis, and recommendation systems, leveraging distributed storage and processing at scale."
" What are two major shortcomings of the original MapReduce model?","(1) Restricts processing to map and reduce steps, missing operations like join or filter. (2) Disk-based intermediate storage is inefficient for iterative tasks such as machine learning."
" How does Apache Spark improve upon MapReduce for iterative tasks?","Spark uses in-memory data storage via Resilient Distributed Datasets (RDDs), reducing repeated disk I/O and significantly improving performance for iterative algorithms."
" Define Resilient Distributed Datasets (RDDs) and their key properties.","RDDs are immutable distributed collections of objects. They can be built from data in storage or from transformations, and they are fault tolerant due to lineage tracking."
" Compare RDDs and DataFrames in Spark.","RDDs provide low-level control, while DataFrames add schema and optimization features. DataFrames are cached and optimized by Sparkâ€™s Catalyst engine, making them faster and more user-friendly for SQL-like operations."
" What is a Directed Acyclic Graph (DAG) in Spark, and why is it important?","A DAG represents the sequence of transformations applied to RDDs. It enables Spark to optimize execution plans and track lineage for fault recovery."
" Differentiate between Narrow and Wide transformations in Spark.","Narrow transformations (e.g., map, filter) only require data from one partition. Wide transformations (e.g., groupByKey) may require data from multiple partitions, triggering shuffles."
" Explain the difference between Spark transformations and actions.","Transformations (e.g., map, filter) create new RDDs but are lazily evaluated. Actions (e.g., collect, count) trigger actual computation and return results to the driver program."
" Provide an example of a Spark operation pipeline using the RDD API.","A word count program: read text from HDFS, split lines into words (flatMap), map each word to (word,1), reduceByKey to sum counts, and save results back to HDFS."
" Name three main use cases of Apache Spark in industry.","Streaming data (real-time analytics), machine learning (iterative algorithms), and data warehousing/interactive analysis."
" Describe a scenario where Spark is not the best choice and why.","For simple batch jobs or when datasets exceed available memory, MapReduce or Hive may be more appropriate because Spark requires careful memory management and is not designed for multi-user environments."
" Explain how Kafka complements distributed storage systems like Hadoop and Spark.","Kafka provides a distributed log of events, enabling real-time data ingestion and streaming pipelines. Hadoop and Spark can consume this data for storage and analytics."
